# Дообучение GPT-2 для задачи генерации историй
В этом репозитории представлен пример дообучения модели GPT-2 для задачи генерации истории по ее описанию. 
## [Источник датасета](https://github.com/facebookresearch/fairseq/tree/main/examples/stories)
Для обучения модели использовалась валидационная выборка из-за большого размера тренеровочной выборки.
## Описание файлов

| Вайл                | Описание
|---------------------|--------------------------------------------
| training.ipynb	  | Ноутбук с дообучением модели
| requirements.txt    | Файл со списком необходимых библиотек 
| train.wp_source     | Описания историй в тренеровочной выборке
| train.wp_target     | Истории в тренеровочной выборке
| test.wp_source      | Описания историй в тестовой выборке
| test.wp_target      | Истории в тестовой выборке